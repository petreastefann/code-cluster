{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### links\n",
    "# https://medium.com/analytics-vidhya/linear-regression-using-pandas-numpy-for-beginners-in-data-science-fe57157ed93d\n",
    "# https://medium.com/berk-hakbilen/regression-in-machine-learning-90a5271a5a12\n",
    "# https://builtin.com/data-science/train-test-split\n",
    "# https://realpython.com/train-test-split-python-data/\n",
    "# https://www.youtube.com/watch?v=nk2CQITm_eo\n",
    "# https://www.youtube.com/watch?v=R15LjD8aCzc\n",
    "# https://www.youtube.com/watch?v=i_LwzRVP7bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T03:42:03.057095Z",
     "start_time": "2022-10-25T03:42:02.093084Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Tuple, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T03:42:03.089163Z",
     "start_time": "2022-10-25T03:42:03.059098Z"
    }
   },
   "outputs": [],
   "source": [
    "# data source: https://www.kaggle.com/datasets/quantbruce/real-estate-price-prediction\n",
    "\n",
    "path = './data/Real estate.csv'\n",
    "full_data = pd.read_csv(path, index_col='No')\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T03:42:03.105140Z",
     "start_time": "2022-10-25T03:42:03.092140Z"
    }
   },
   "outputs": [],
   "source": [
    "# we drop the column 'X1 transaction date'\n",
    "full_data.drop(labels='X1 transaction date', axis=1, inplace=True)\t#inplace=True - modifies the original object\n",
    "\n",
    "#### solution for the requirement at the bottom of the file..\n",
    "#full_data.drop(labels=['X5 latitude', 'X6 longitude'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting subsets of training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial dataset is divided in two: 70% is the training of the model - the establishment of weight $\\theta$, and the remaining is for testing, in order to see how generalized the model is.\n",
    "\n",
    "As the initial order of the data may disadvantage the model (in the data above, the first records are the oldest sales; the most recent ones are the last in the file), we decide to shuffle the data beforehand and then divide it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T03:42:03.121139Z",
     "start_time": "2022-10-25T03:42:03.107141Z"
    }
   },
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# randomly shuffle the data\n",
    "full_data = full_data.sample(frac=1)\n",
    "# get the percentage of train data ==> test data = 1 - train data\n",
    "percent_train: float = 0.7\n",
    "length_of_train: int = int(len(full_data) * percent_train)\n",
    "\n",
    "# split the data into train and test\n",
    "train: np.array = np.array(full_data[:length_of_train])\n",
    "test: np.array = np.array(full_data[length_of_train:])\n",
    "\n",
    "assert isinstance(train, np.ndarray), 'Train set should be an array'\n",
    "assert isinstance(test, np.ndarray), 'Test set should be an array'\n",
    "assert len(train) + len(test) == len(full_data)\n",
    "assert train.shape[1] == test.shape[1] == len(full_data.columns), 'The number of columns should be the same'\n",
    "\n",
    "\n",
    "print(len(full_data), \"=\", len(train), \"+\", len(test), \"(length of full_data = length of train + length of test)\\n\")\n",
    "\n",
    "print(\"--> full_data[4]:\\n\", tabulate([full_data.iloc[4]], headers='keys', tablefmt='plain-text', showindex=False))\n",
    "print(\"--> train[4]:\\n\", train[4], '\\n\\n')\n",
    "\n",
    "print(\"--> full_data[{}]=\".format(length_of_train+3))\n",
    "print(tabulate([full_data.iloc[length_of_train+3]], headers='keys', tablefmt='plain-text', showindex=False))\n",
    "print(\"--> test[3]:\\n\", test[3])\n",
    "\n",
    "####links\n",
    "#https://www.statology.org/pandas-train-test/\n",
    "#https://stackoverflow.com/questions/24147278/how-do-i-create-test-and-train-samples-from-one-dataframe-with-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T03:42:03.137147Z",
     "start_time": "2022-10-25T03:42:03.123147Z"
    }
   },
   "outputs": [],
   "source": [
    "# split the train/test data into x_train/x_test and their 'y' validation (x columns are input, y columns are output)\n",
    "# in our case the x columns are those with info from which we deduce the price (y column)\n",
    "x_train, y_train = train[:, :-1], train[:, -1]\n",
    "x_test, y_test = test[:, :-1], test[:, -1]\n",
    "# y columns have a shape of (n,) so we need to reshape them to (n, 1) to be able to use them in the model\n",
    "y_train = np.transpose([y_train])\n",
    "y_test = np.transpose([y_test])\n",
    "\n",
    "assert x_train.shape[0] == train.shape[0]\n",
    "assert x_test.shape[0] == test.shape[0]\n",
    "assert y_train.shape == (x_train.shape[0], 1), f'The vector y_train is a column vector'\n",
    "assert y_test.shape == (x_test.shape[0], 1), f'The vector y_test is a column vector'\n",
    "\n",
    "\n",
    "print(\"-> train:\\n\", train[1])\n",
    "print(\"-> x_train + y_train:\\n\", x_train[1], y_train[1])\n",
    "print(\"-> x_train.shape + y_train.shape: \", x_train.shape, y_train.shape)\n",
    "print()\n",
    "print(\"-> test:\\n\", test[1])\n",
    "print(\"-> x_test + y_test:\\n\", x_test[1], y_test[1])\n",
    "print(\"-> x_test.shape + y_test.shape: \", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions below are used for the convenient transformation of the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the scaling of the values from the training set between 0 and 1 it is needed to know the min and max of every attribute. For a matrix of form:\n",
    "$$\n",
    "\\mathbf{X} = \n",
    "\\begin{pmatrix}\n",
    "x_0^{(1)} & x_1^{(1)} & x_2^{(1)} & \\dots & x_n^{(1)}\n",
    "\\\\\n",
    "x_0^{(2)} & x_1^{(2)} & x_2^{(2)} & \\dots & x_n^{(2)}\n",
    "\\\\\n",
    "\\vdots & \\vdots & \\vdots\t& \\vdots & \\vdots\n",
    "\\\\\n",
    "x_0^{(m)} & x_1^{(m)} & x_2^{(m)} & \\dots & x_n^{(m)}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "the next are receieved: the minimum and maximum of every column $min_i$, $max_i$, from every value the min of its column is extracted and the value is divided by the difference between the max and the min of the same column. it is important to notice the fact that a column may be constant(min and max are both 1 in column 0 in the design matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_max_cols(mat: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"For a given matrix, calculate and return the maximums and minimums of each column.\n",
    "    param mat: matrix with at least two rows\n",
    "    return: minimums and maximums of columns.\"\"\"\n",
    "    # mins, maxes = [min(i) for i in mat.T], [max(i) for i in mat.T]\n",
    "    mins, maxes = mat.min(axis=0, keepdims=True), mat.max(axis=0, keepdims=True) \n",
    "    # it is recommended to return the minimums and maximums as a matrix with one line.\n",
    "    assert mins.shape == maxes.shape == (1, mat.shape[1])\n",
    "    return mins, maxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The design matrix has the value 1 on the first column to allow a free term. the rest of the columns are the read data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T03:42:04.137140Z",
     "start_time": "2022-10-25T03:42:04.124140Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_design_matrix(mat: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"The function creates a design matrix from a given matrix. The design matrix has the same number of \n",
    "    rows as the given one, but with a column of ones added(at the beginning)\"\"\"\n",
    "    result: np.ndarray=np.insert(mat, 0, 1, axis=1)\n",
    "    assert result.shape == (mat.shape[0], mat.shape[1]+1)\n",
    "    assert np.all(result[:, 0] == 1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the scaling of data in the interval $[0, 1]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_values(mat: np.ndarray, min_cols: np.ndarray, max_cols: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Being given the minimums and maximums of each column, scale the values of the given matrix between 0 and 1\n",
    "    the formula is: (x - min) / (max - min). The minimums and maximums are calculated for each column separately.\"\"\"\n",
    "    \n",
    "    assert isinstance(mat, np.ndarray) and mat.ndim == 2, 'mat must be a matrix'\n",
    "    assert mat.shape[0] > 1, 'at least two columns are required'\n",
    "    assert mat.shape[1] == min_cols.shape[-1] == max_cols.shape[-1]\n",
    "    \n",
    "    # the case where a column is constant is treated, i.e. the minimum and maximum on it coincide.\n",
    "    # these columns will not make the division, as it would result in a division by 0\n",
    "    \n",
    "    result=mat;\n",
    "    for i_column in range(result.shape[1]):\n",
    "        if result[:, i_column].min() == result[:, i_column].max():\n",
    "            continue\n",
    "        result[:, i_column]=(result[:, i_column]-min_cols[0, i_column])/(max_cols[0, i_column]-min_cols[0, i_column])\n",
    "        \n",
    "    assert result.shape == mat.shape, 'Forma matricei se pastreaza'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T03:42:04.233139Z",
     "start_time": "2022-10-25T03:42:04.140149Z"
    }
   },
   "outputs": [],
   "source": [
    "# code testing\n",
    "\n",
    "_mat: np.ndarray = np.random.randn(1000, 1000) * 50\n",
    "min_mat, max_mat = get_min_max_cols(_mat)\n",
    "_mat_scaled: np.ndarray = scale_values(_mat, min_mat, max_mat)\n",
    "\n",
    "epsilon: float = 1e-5  # a very small natural number; the calculations in floating point might get to a point where the result is not exactly 0 or 1\n",
    "\n",
    "assert np.min(_mat_scaled) >= -epsilon, f'The matrix is not scaled in [0, 1]: its minimum is {np.min(_mat_scaled)}'\n",
    "assert np.max(_mat_scaled) <= 1 + epsilon, f'The matrix is not scaled in [0, 1]: its maximum is {np.max(_mat_scaled)}'\n",
    "\n",
    "_mat = get_design_matrix(_mat)  # add a column of ones\n",
    "min_mat, max_mat = get_min_max_cols(_mat)\n",
    "_mat_scaled = scale_values(_mat, min_mat, max_mat)\n",
    "\n",
    "print(_mat_scaled)\n",
    "print(np.min(_mat_scaled), np.max(_mat_scaled))\n",
    "\n",
    "assert np.min(_mat_scaled) >= -epsilon, f'The matrix is not scaled in [0, 1]: its minimum is {np.min(_mat_scaled)}'\n",
    "assert np.max(_mat_scaled) <= 1 + epsilon, f'The matrix is not scaled in [0, 1]: its maximum is {np.max(_mat_scaled)}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set is preprocessed: design matrix -> scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T03:42:22.184487Z",
     "start_time": "2022-10-25T03:42:22.160419Z"
    }
   },
   "outputs": [],
   "source": [
    "# getting the design matrix\n",
    "x_train: np.ndarray = get_design_matrix(x_train)\n",
    "\n",
    "# we calculate the minimums and maximums on column, from the training set\n",
    "# these will be used for transforming the data from the test set\n",
    "x_train_min, x_train_max = get_min_max_cols(x_train)\n",
    "\n",
    "# we scale the data in the interval [0, 1], using scale_values\n",
    "x_train_scaled: np.ndarray = scale_values(x_train, x_train_min, x_train_max)\n",
    "\n",
    "epsilon: float = 1e-5  # a very small natural number\n",
    "\n",
    "assert np.min(x_train_scaled) >= -epsilon, f'The matrix is not scaled in [0, 1]: its minimum is {np.min(_mat_scaled)}'\n",
    "assert np.max(x_train_scaled) <= 1 + epsilon, f'The matrix is not scaled in [0, 1]: its maximum is {np.max(_mat_scaled)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the test set the same type of preprocessing is done, with the observation that for scaling the values from `x_train_min` and `x_train_max` will be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test: np.ndarray = get_design_matrix(x_test)\n",
    "\n",
    "x_test_scaled: np.ndarray = scale_values(x_test, x_train_min, x_train_max)\n",
    "\n",
    "print(f'The minimum values from every column of the testing set should not be very much different from 0, except the first column')\n",
    "print(f'all column minimums for the testing set: {np.min(x_test_scaled)}')\n",
    "\n",
    "print(f'The maximum values from every column of the testing set should not be very much different from 1, except from the first column, which should actually be 1')\n",
    "print(f'all column maximums for the testing set: {np.max(x_test_scaled)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model of the prediction is:\n",
    "$$\n",
    "h_\\theta(X) = X \\cdot \\theta\n",
    "$$\n",
    "where $X$ is a design matrix and $\\theta$ is a weight vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T03:42:23.098233Z",
     "start_time": "2022-10-25T03:42:23.079152Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_predict(theta: np.ndarray, x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Makes predictions for the values from the given matrix x, using the given theta.\n",
    "    :param theta: column matrix of weights\n",
    "    :param x: design matrix. every line contains the values of the atributes necessary for a registration.\n",
    "    :return: column matrix of predictions, with the same number of rows as :param x:.\"\"\"\n",
    "    # numarul de trasaturi din x este egal cu numarul de coeficienti din theta\n",
    "    assert x.shape[1] == theta.shape[0]\n",
    "    # vectorul theta e vector coloana\n",
    "    assert theta.shape[1] == 1\n",
    "    result: np.ndarray = np.dot(x, theta)\n",
    "    assert result.shape == (x.shape[0], 1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T03:42:23.475072Z",
     "start_time": "2022-10-25T03:42:23.467977Z"
    }
   },
   "outputs": [],
   "source": [
    "# verifying\n",
    "\n",
    "np.random.seed(100)\n",
    "\n",
    "_x: np.ndarray = np.random.rand(3, 2)\n",
    "_y: np.ndarray = np.random.rand(3, 1)\n",
    "_theta: np.ndarray = np.ones((2, 1))\n",
    "\n",
    "assert np.allclose(model_predict(_theta, _x), np.array([[0.82177433],\n",
    "       [1.26929372],\n",
    "       [0.12628798]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function measures how different are the predicitons from the known values (ground truth). Here we use as function of error half of the mean squared error (https://en.wikipedia.org/wiki/Mean_squared_error):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T03:42:23.867045Z",
     "start_time": "2022-10-25T03:42:23.858040Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def loss(theta: np.ndarray, x: np.ndarray, y: np.ndarray) -> float:\n",
    "    \"\"\"Error function used for gradient descent.\n",
    "    :param theta: column matrix of weights\n",
    "    :param x: design matrix. every line contains the values of the atributes necessary for a registration.\n",
    "    :param y: output values, corresponding to every registration from x\n",
    "    :return: scalar value, representing the error\"\"\"\n",
    "    \n",
    "    m: int = x.shape[0]\n",
    "    y_hat: np.ndarray = model_predict(theta, x)\n",
    "    # result = mean_squared_error(y_hat, y)\n",
    "    result: np.float64 = 1/(2*m) * np.sum((y_hat - y)**2)   # why is it not 1/m instead?\n",
    "    return result\n",
    "\n",
    "# https://datagy.io/mean-squared-error-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T03:42:24.324008Z",
     "start_time": "2022-10-25T03:42:24.313633Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "\n",
    "_x: np.ndarray = np.random.rand(3, 2)\n",
    "_y: np.ndarray = np.random.rand(3, 1)\n",
    "_theta: np.ndarray = np.ones((2, 1))\n",
    "\n",
    "assert np.allclose(loss(_theta, _x, _y), 0.03659284388808936)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The gradient descent and training calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T03:42:24.874457Z",
     "start_time": "2022-10-25T03:42:24.866468Z"
    }
   },
   "outputs": [],
   "source": [
    "def gradient(theta: np.ndarray, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Calculates the gradient of the loss function, for the given theta, x and y.\n",
    "    :param theta: column matrix of weights\n",
    "    :param x: design matrix with the entry values\n",
    "    :param y: output values\n",
    "    :return: vector of output values\"\"\"\n",
    "    \n",
    "    assert theta.shape[0] == x.shape[1]\n",
    "    m: int = x.shape[0]\n",
    "    y_hat: np.ndarray = model_predict(theta, x)\n",
    "    assert y_hat.shape == (m, 1)\n",
    "    grad: np.ndarray = 1/m * np.dot(x.T, (y_hat - y))\n",
    "    assert grad.shape == theta.shape\n",
    "    return grad\n",
    "\n",
    "# https://pythonocean.com/blogs/linear-regression-using-gradient-descent-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T03:42:25.596006Z",
     "start_time": "2022-10-25T03:42:25.576936Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(x: np.ndarray, y: np.ndarray, alpha:float=0.1, max_iters = 10000, verbose=True) -> Tuple[np.ndarray, List[float]]:\n",
    "    \"\"\"Trains the linear regression model on the given data x and y. Returns a list of error values and the final theta.\n",
    "    :param x: design matrix with entry values\n",
    "    :param y: output values\n",
    "    :param alpha: learning rate, implicitly value is 0.1\n",
    "    :param max_iters: the maximum number of allowed iterations. if the treshold is reached, the training stops.\n",
    "    :param verbose: if True, prints the error after every 1000 iterations: the number of iterations and the error value.\n",
    "    :return: a tuple containing the final weights(theta) and a list of error values\"\"\"\n",
    "    \n",
    "    eps: float = 1e-6\n",
    "    assert x.shape[0] == y.shape[0]\n",
    "    assert np.all(x[:, 0] == 1), 'First column of x has to be all ones'\n",
    "    m, n = x.shape\n",
    "    theta: np.ndarray = np.zeros((n, 1))\n",
    "    assert theta.shape == (n, 1)\n",
    "    assert np.all(theta==0)\n",
    "    \n",
    "    losses: list = []\n",
    "    losses.append(loss(theta, x, y))\n",
    "    iters: int = 0\n",
    "    while True:\n",
    "        iters += 1\n",
    "        theta = theta - alpha * gradient(theta, x, y)\n",
    "        current_loss = loss(theta, x, y)\n",
    "        losses.append(current_loss)\n",
    "        if (np.abs(losses[-1] - losses[-2]) < eps) or (iters > max_iters):\n",
    "            break\n",
    "        if np.isnan(current_loss) or np.isinf(current_loss):\n",
    "            break\n",
    "        if verbose and iters % 1000 == 0:\n",
    "            print(f'Iteration {iters}; loss = {current_loss}')\n",
    "    return theta, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T03:42:26.336205Z",
     "start_time": "2022-10-25T03:42:26.133402Z"
    }
   },
   "outputs": [],
   "source": [
    "theta, losses = train(x_train_scaled, y_train, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'losses={losses}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T03:42:26.919093Z",
     "start_time": "2022-10-25T03:42:26.770656Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean squared error function\n",
    "\n",
    "def mse(y_true: np.ndarray, y_hat: np.ndarray) -> float:\n",
    "    \"\"\"Calculates mean squared error for the vectors y_true and y_hat.\n",
    "    :param y_true: vector of true values\n",
    "    :param y_hat: vector of predicted values\n",
    "    :param return: mean squared error\"\"\"\n",
    "    \n",
    "    return 1/y_true.shape[0] * np.sum((y_true - y_hat)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train: np.ndarray = model_predict(theta, x_train_scaled)\n",
    "    \n",
    "print(f'Mean squared error for the training set: {mse(y_hat_train, y_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Actual and predicted values, training set: {list(zip(y_train, y_hat_train))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test: np.ndarray = model_predict(theta, x_test_scaled)\n",
    "    \n",
    "print(f'Mean squared error for the testing set: {mse(y_hat_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'actual and predicted values, testing set: {list(zip(y_test, y_hat_test))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements:\n",
    "1. Find a value for alpha (learning rate) for which the model produces a list of ascending values for the loss function. Make a plot of the loss function \n",
    "2. Find a value for alpha > 0 for which the algorithm doesnt stop in 'max_iters' iterations. Make a plot of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution for requirement 1\n",
    "theta, losses = train(x_train_scaled, y_train, alpha = -0.0004, max_iters = 7300, verbose = True)\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution for requirement 2\n",
    "theta, losses = train(x_train_scaled, y_train, alpha=0.25, max_iters = 10000, verbose = True)\n",
    "losses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The normal equation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For linear regression the weights $\\theta$ can be found by solving the following equation:\n",
    "$$\n",
    "{\\theta}^{(min)} = \\left(\\mathbf{X}^t \\mathbf{X}\\right)^{-1}\\mathbf{X}^t\\cdot \\mathbf{y}\n",
    "$$\n",
    "where $\\mathbf{X}$ is the design matrix, $\\mathbf{y}$ is the vector of ground truth values (known values).\n",
    "For determining the pseudoinverse of $\\left(\\mathbf{X}^t \\mathbf{X}\\right)^{-1}\\mathbf{X}^t$ of matrix $\\mathbf{X}$ we use the function `np.linalg.pinv` from [NumPy](https://numpy.org/doc/stable/reference/generated/numpy.linalg.pinv.html).\n",
    "\n",
    "Determine the weights vector using the pseudoinverse and compare the weights vector with the one obtained using gradient descent, for an $\\alpha$ which makes the loss function to decrease.\n",
    "\n",
    "Obs: in the normal equation it is not needed to have the data scaled/normalized, as opposed to the gradient descent algorithm. for comparison purposes however, use the scaled matrix `X_train_scaled`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_normal: np.ndarray = np.linalg.inv(x_train_scaled.T @ x_train_scaled) @ x_train_scaled.T @ y_train\n",
    "print(f'The weights vector determined by the normal equation method is:\\n{theta_normal}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we determine a weights vector with the gradient descent method, using the training set\n",
    "theta_gradient, _ = train(x_train_scaled, y_train, alpha=0.05, max_iters=10000, verbose=False)\n",
    "print(f'The weights vector determined by the gradient descent method is:\\n{theta_gradient}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The distance between the two determined vectors (by using the two previous methods): {np.linalg.norm(theta_gradient - theta_normal)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The selection of attributes\n",
    "In the initial dataset the latitude and logitude are present as columns x5 and x6, being the last two columns in x_train and x_test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ask outselves if removing the two column improves the performance of the prediction model. Implement and execute the following steps:\n",
    "1. Delete the last two columns from x_train and x_test;\n",
    "2. Re-call the preprocessing functions for the new x_train and x_test;\n",
    "3. Re-train the model using the gradient descent algorithm and calculate the loss function on the training and testing sets. Compare the values obtained with the ones obtained in the initial case;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "264.432px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "58f10e1b20caf39f3bae4facf866a1e85e14779671631b268474442cf9b2784d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
