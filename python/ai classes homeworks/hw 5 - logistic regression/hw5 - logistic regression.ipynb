{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (unpack the data.rar file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "useful resources:\n",
    "\n",
    "[1] [Cross entropy for dummies](https://towardsdatascience.com/cross-entropy-for-dummies-5189303c7735)\n",
    "\n",
    "[2] [Understanding logistic regression](https://towardsdatascience.com/understanding-logistic-regression-step-by-step-704a78be7e0a)\n",
    "\n",
    "[3] [Cross entropy log loss and intuition behind it](https://towardsdatascience.com/cross-entropy-log-loss-and-intuition-behind-it-364558dca514)\n",
    "\n",
    "[4] [Cross entropy (see the section \"Relation with log-likelihood\")](https://en.wikipedia.org/wiki/Cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-16T20:13:48.848256Z",
     "start_time": "2022-03-16T20:13:47.917023Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import functools\n",
    "import pandas as pd\n",
    "from typing import Union\n",
    "\n",
    "plt.rc('font', **{'size' : 18})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-16T20:13:50.937255Z",
     "start_time": "2022-03-16T20:13:48.849247Z"
    }
   },
   "outputs": [],
   "source": [
    "import tableprint as tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Linear regression 'matches' a linear (polinomial) function using a set of data $X$, on the whole domain $\\mathbb{R}$, $linreg(x) : \\mathbb{R}^m \\rightarrow \\mathbb{R}$. The logistic regression has the same domain, however the codomain is way more limited, namely $logreg(x) : \\mathbb{R}^n \\rightarrow (0, 1)$. It tries to predict the probability that the element $x \\in X$ is to be part of the positive class.\n",
    "\n",
    "This probability is marked as $P(y=1|x,\\theta)$, and we take it as being the probability associated to the feedback $x^T \\theta$ calculated by the classic regression, under the circumstance that we know the features $X$ and the parameters $\\theta$ of the model.\n",
    "\n",
    "The ideea is that for every entry $x$, the logistic regression model pairs a probability. We will show how we choose the function that calculates the probability using the result of the linear regression $x^T \\theta$.\n",
    "\n",
    "We start from a function that has as parameter a probability and that maps the interval $(0, 1)$ on the whole real axis $\\mathbb{R}$. We observe how the function $f_1(p) = \\frac{p}{1-p}$ maps the probability in $\\mathbb{R}+$, and if we apply the logarithm, the function $f_2(p) = log\\left(\\frac{p}{1-p}\\right)$ maps the interval $(0, 1)$ on the whole real axis $\\mathbb{R}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-16T20:13:53.314156Z",
     "start_time": "2022-03-16T20:13:52.946546Z"
    }
   },
   "outputs": [],
   "source": [
    "f1 = lambda x: x / (1 - x)\n",
    "f2 = lambda x: np.log(x / (1 - x))\n",
    "x:np.array = np.linspace(1e-4, 1-1e-4, 100)\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(20, 10), sharex=True)\n",
    "ax[0].plot(x, f1(x)) \n",
    "ax[1].plot(x, f2(x))\n",
    "ax[0].set_ylabel(r'$f_1(p) = \\frac{p}{1-p}$') \n",
    "ax[0].grid()\n",
    "ax[1].set_xlabel('p') \n",
    "ax[1].set_ylabel(r'$f_2(p) = log\\left(\\frac{p}{1-p}\\right)$') \n",
    "ax[1].grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that if the answer given by the regression is 0 ($f_2(p) = 0$), the probability associated is 0.5. We want the response calculated by the regression (which can be any value on the real axis) to be equal with this function of probability:\n",
    "\n",
    "$$h_{\\theta}(x) = x^T \\theta = log\\left(\\frac{p}{1-p}\\right)$$\n",
    "\n",
    "Onwards, with a few algebraic conversions, we can prove that:\n",
    "\n",
    "$$\\hat{y} = P(y=1|x, \\theta) = p = \\frac{1}{1 + e^{-h_{\\theta}(x)}}$$\n",
    "\n",
    "Thus we have a function that maps $x$ and the parameters of the model $\\theta$ in a probability. The function is named as logistic function.\n",
    "\n",
    "In reality, we have the dataset $X$, and a binary value $y_i \\in \\{0, 1\\}$ associated to every $x_i$, $i=1 \\dots m$. Actually, the dataset $X$ is described by a very simple probability distribution, a distribution in which the probabilities can take only the values 0 or 1. If we put in the values $x_i$ on the x axis and the values $y_i$ on the y axis, the distribution graph would be very \"damaged\" - after all, y takes only 2 values.\n",
    "\n",
    "We want the model defined by the coefficients $\\theta$ to 'fit' as good as possible over this initial given distribution - perhaps the \"landscape\" of the distribution function of the model is not so rough anymore, but smoother (continuous). Of course this example is exaggerated, in reality the entry space $X$ is $m$ dimansional, not 1-dimensional as we have assumed here that we imagine the representation of probability distribution.\n",
    "\n",
    "However, in order to compare 'how good' the distribution of the model that we learn fits with the initial distribution, we need a measure of these distributions. For this we will introduce the notion of entropy as measure of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The binomial logistic regression\n",
    "The cost function for the logistic regression is given by the binary cross-entropy, written in a vectorial way as such:\n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\left(\\mathbf{y}^t \\cdot \\ln \\hat{\\mathbf{y}} + (1-\\mathbf{y})^t \\cdot \\ln ( \\mathbb{1}_m - \\hat{\\mathbf{y}}) \\right) \n",
    "$$\n",
    "\n",
    "The gradient of the cost function is:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta} J(\\theta) = \\frac{1}{m} \\mathbf{X}^t (\\hat{\\mathbf{y}} - \\mathbf{y})\n",
    "$$\n",
    "\n",
    "The change of weight from the vector $\\boldsymbol{\\theta}$ is made at every iteration (epoch) with:\n",
    "$$\n",
    "\\boldsymbol{\\theta} = \\boldsymbol{\\theta} - \\alpha\\frac{1}{m} \\mathbf{X}^t (\\hat{\\mathbf{y}} - \\mathbf{y})\n",
    "$$\n",
    "with $\\alpha > 0$ as learning rate. \n",
    "\n",
    "https://kisaragiry.medium.com/binomial-logistic-regression-math-explained-c2569cdbd2c5\n",
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-17T08:07:36.526101Z",
     "start_time": "2022-03-17T08:07:32.941186Z"
    }
   },
   "outputs": [],
   "source": [
    "path_train: str = './data/mnist_train.csv'\n",
    "path_test: str = './data/mnist_test.csv'\n",
    "\n",
    "train_set: np.ndarray = pd.read_csv(path_train, header=None).values\n",
    "test_set: np.ndarray = pd.read_csv(path_test, header=None).values\n",
    "\n",
    "assert(train_set.shape) == (60000, 785)\n",
    "assert(test_set.shape) == (10000, 785)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-17T08:07:37.066306Z",
     "start_time": "2022-03-17T08:07:36.528095Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_set and test_set are matrixes that contain on the first column the class (a digit from 0 to 9),\n",
    "#  and the image is being held starting from the column 1 to the end.\n",
    "# take into train_x only the images and into train_y only the class, \n",
    "# and do the same for test_x and test_y\n",
    "\n",
    "train_x, train_y = train_set[:, 1:], train_set[:, 0]\n",
    "test_x, test_y = test_set[:, 1:], test_set[:, 0]\n",
    "\n",
    "assert train_x.shape == (60000, 784)\n",
    "assert train_y.shape == (60000,)\n",
    "assert test_x.shape == (10000, 784)\n",
    "assert test_y.shape == (10000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see the dataset. We will observe the first 16 lines from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-16T19:12:03.950581Z",
     "start_time": "2022-03-16T19:12:03.544283Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_samples(x_set: np.ndarray, y_set: np.array) -> None:\n",
    "    size: int = x_set.shape[0]\n",
    "    \n",
    "    fig, ax = plt.subplots(size // 4, 4, figsize=(20, 10))\n",
    "    for k in range(size):\n",
    "        row, col = k // 4, k % 4\n",
    "        \n",
    "        # Make those columns into a array of 8-bits pixels\n",
    "        # The pixel intensity values are integers from 0 to 255\n",
    "        pixels = np.array(x_set[k], dtype='uint8')    \n",
    "        \n",
    "        # Reshape the array into 28 x 28 array (2-dimensional array)\n",
    "        n = int(np.sqrt(len(pixels)))\n",
    "        assert n**2 == len(pixels)\n",
    "        pixels = pixels.reshape(n, n)\n",
    "        ax[row, col].imshow(pixels, cmap='gray')\n",
    "        ax[row, col].set_title('Label {label}'.format(label=y_set[k]))\n",
    "        ax[row, col].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "show_samples(train_x[:16, :], train_y[:16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the binomial regression we care about classifying only the relevant images of two classes, like the digits '0' and '1' (for now). We will define the sets that 'crop' only these two classes from the given sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter from the big sets only the samples suitable for the digits 0 and 1\n",
    "# determine the logical vectors with True for the indexes for which the classes from the vectors y are 0 or 1 and False otherwise.\n",
    "\n",
    "labels_0_1_train: np.ndarray = np.logical_or(train_y == 0, train_y == 1)\n",
    "labels_0_1_test: np.ndarray = np.logical_or(test_y == 0, test_y == 1)\n",
    "\n",
    "print(labels_0_1_train, '\\n', labels_0_1_test, '\\n')\n",
    "\n",
    "assert labels_0_1_train.shape == (60000,)\n",
    "assert labels_0_1_test.shape == (10000,)\n",
    "\n",
    "assert labels_0_1_train.sum() == 12665\n",
    "assert labels_0_1_test.sum() == 2115\n",
    "\n",
    "# filter from the big sets only the samples suitable for the digits 0 and 1\n",
    "# you can use reshape for the vectors y\n",
    "# use the logical vectors labels_0_1_train and labels_0_1_test for the filtering\n",
    "train_x_bin, train_y_bin = train_x[labels_0_1_train,\n",
    "                                   :], train_y[labels_0_1_train].reshape(-1, 1)\n",
    "test_x_bin, test_y_bin = test_x[labels_0_1_test,\n",
    "                                :], test_y[labels_0_1_test].reshape(-1, 1)\n",
    "\n",
    "print(train_x_bin, '\\n\\n', train_y_bin, '\\n')\n",
    "print(test_x_bin, '\\n\\n', test_y_bin)\n",
    "\n",
    "assert train_x_bin.shape == (12665, 784)\n",
    "assert train_y_bin.shape == (12665, 1)\n",
    "assert test_x_bin.shape == (2115, 784)\n",
    "assert test_y_bin.shape == (2115, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the linear regression, the first column needs to be made of ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ones_column(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Returns a matrix with first column filled with 1 and the other columns being x's columns. \"\"\"\n",
    "    return np.concatenate((np.ones(shape = (x.shape[0], 1)), x), axis = 1)\n",
    "\n",
    "train_x_bin_ext: np.ndarray = add_ones_column(train_x_bin)\n",
    "test_x_bin_ext: np.ndarray = add_ones_column(test_x_bin)\n",
    "\n",
    "assert train_x_bin_ext.shape == (12665, 785)\n",
    "assert test_x_bin_ext.shape == (2115, 785)\n",
    "assert np.all(train_x_bin_ext[:, 0] == 1)\n",
    "assert np.all(test_x_bin_ext[:, 0] == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features need to be normalized, the maximum value being 255. The normalization follows as the resulting features to be between the interval \\[0, 1\\], so we divite to the maximum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-16T17:29:25.610016Z",
     "start_time": "2022-03-16T17:29:25.570018Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Normalization means division by 255.\n",
    "    Args:\n",
    "        x: feature matrix, shape m * n. It will not be changed by this code.\n",
    "    Returns:\n",
    "        matrix with scaled values between 0 and 1, of same shape\"\"\"\n",
    "    return x/255\n",
    "\n",
    "train_x_bin_ext: np.ndarray = add_ones_column(normalize(train_x_bin))\n",
    "test_x_bin_ext: np.ndarray = add_ones_column(normalize(test_x_bin))\n",
    "\n",
    "assert train_x_bin_ext.shape == (12665, 785)\n",
    "assert test_x_bin_ext.shape == (2115, 785)\n",
    "assert np.all(train_x_bin_ext[:, 0] == 1)\n",
    "assert np.all(test_x_bin_ext[:, 0] == 1)\n",
    "assert np.all(train_x_bin_ext <= 1)\n",
    "assert np.all(test_x_bin_ext <= 1)\n",
    "assert np.all(train_x_bin_ext >= 0)\n",
    "assert np.all(test_x_bin_ext >= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the sigmoid function, $sigmoid(z) = \\frac{1}{1 + e^{-z}}$ respectively $\\hat y = h(x, \\theta) = sigmoid \\left( X  \\theta \\right)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z: Union[float, np.ndarray]) -> Union[float, np.ndarray]:\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "assert sigmoid(0) == 0.5\n",
    "assert np.abs(sigmoid(1) - 0.731058) < 1e-6\n",
    "\n",
    "def h(x: Union[float, np.ndarray], theta: Union[float, np.ndarray]) -> Union[float, np.ndarray]:\n",
    "    return sigmoid(np.dot(x, theta))\n",
    "\n",
    "assert np.abs(h(np.array([1., 1., 1., 0]), np.array([1., 0., 1., 1.])) - 0.880797) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the cost function with adjustment this time (careful, the coefficient $\\theta_0$ does not adjust):\n",
    "\n",
    "\\begin{align}\n",
    "J(\\boldsymbol\\theta) & = \\underbrace{-\\frac{1}{m} \\sum\\limits_{i=1}^m \\left[y^{(i)}\\cdot \\ln h_{\\boldsymbol\\theta}(\\mathbf{x}^{(i)}) + (1-y^{(i)})\\cdot \\ln (1- h_{\\boldsymbol\\theta}(\\mathbf{x}^{(i)})) \\right]}_{\\text{error of quality}} \n",
    "\\\\\n",
    "& + \\underbrace{\\frac{\\lambda}{2} \\sum\\limits_{j=1}^n \\theta_j^2}_{\\text{term of adjustment}}\n",
    "%\\label{eq:JCrossEntropyRegularizare}\n",
    "\\\\\n",
    "& = \n",
    "\\underbrace{-\\frac{1}{m} \\sum\\limits_{i=1}^m \\left[y^{(i)}\\cdot \\ln \\hat y^{(i)}  + (1-y^{(i)})\\cdot \\ln (1- \\hat y^{(i)}) \\right]}_{\\text{error of quality}} \n",
    "\\\\\n",
    "& + \\underbrace{\\frac{\\lambda}{2} \\sum\\limits_{j=1}^n \\theta_j^2}_{\\text{term of adjustment}}\n",
    "\\\\\n",
    "& = -\\frac{1}{m} \\left(\\mathbf{y}^t \\cdot \\ln\\hat{\\mathbf{y}}  + (\\mathbb{1}-\\mathbf{y})^t \\cdot \\ln(\\mathbb{1}-\\hat{\\mathbf{y}})\\right) + \\frac{\\lambda}{2} \\| \\boldsymbol\\theta[1:]  \\|^2_2\n",
    "\\end{align}\n",
    "\n",
    "where $\\boldsymbol\\theta[1:]$ is the vector made from all the components of $\\boldsymbol\\theta$ excluding the first one, and $\\| \\mathbf{v} \\|_2$ is the Euclidean norm of the vector $\\mathbf{v}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-16T18:29:22.708203Z",
     "start_time": "2022-03-16T18:29:22.674203Z"
    }
   },
   "outputs": [],
   "source": [
    "def cost(x: np.ndarray, y: np.ndarray, theta: np.ndarray, lmbda: float) -> float:\n",
    "    \"\"\" Cost function includes also regularization\n",
    "    Args:\n",
    "        x: the feature matrix, dimension m x (n+1)\n",
    "        y: the evidence vector, dimension m x 1\n",
    "        theta: the vector of coefficients, dimension (n+1) x 1\n",
    "    Returns:\n",
    "        the cost, as a scalar\"\"\"\n",
    "    assert x.shape[1] == theta.shape[0]\n",
    "    assert x.shape[0] == y.shape[0]\n",
    "    assert theta.shape[1] == y.shape[1] == 1\n",
    "    assert lmbda >= 0\n",
    "    # calculating y_hat:\n",
    "    y_hat: any = h(x, theta)\n",
    "    # calculating the error of quality:\n",
    "    m: int = x.shape[0]\n",
    "    j1: float = -1/m * np.sum(y * np.log(y_hat) + (1-y) * np.log(1-y_hat))\n",
    "    # calculating the error of adjustment:\n",
    "    j2: float = lmbda/(2) * np.sum(theta[1:]**2)\n",
    "    print(j1 + j2)\n",
    "    return j1 + j2\n",
    "    \n",
    "\n",
    "np.random.seed(11)\n",
    "n: int = train_x_bin_ext.shape[1]\n",
    "theta: np.ndarray = np.random.randn(n).reshape(n, 1)\n",
    "assert np.abs(cost(train_x_bin_ext, train_y_bin, theta=theta, lmbda=0.2) - 79.815566) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the gradient, using the expression defined previously, taking into account the term of adjustment:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}) = \\frac{1}{m} \\mathbf{X}^t(\\hat{\\mathbf{y}} - \\mathbf{y}) + \\lambda (0, \\theta_1, \\dots, \\theta_n)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(x: np.ndarray, y: np.ndarray, theta: np.ndarray, lmbda: float) -> np.array:\n",
    "    \"\"\" Computation of gradient with regularization term\n",
    "    \n",
    "    Args:\n",
    "        x: the feature matrix, dimension m x (n+1)\n",
    "        y: the evidence vector, dimension m\n",
    "        theta: the vector of coefficients, dimension n+1\n",
    "        lmbda: the coefficient of adjustment\n",
    "    Returns:\n",
    "        the gradient,as vector of n+1 elements\"\"\"\n",
    "    m: int = x.shape[0]\n",
    "    y_hat: any = h(x, theta)\n",
    "    g1: np.array = (1/m) * np.dot(x.T, (y_hat - y)) # the gradient of quality, without adjustment\n",
    "    \n",
    "    theta_simple = theta.copy()\n",
    "    theta_simple[0] = 0\n",
    "    # the gradient related to the term of adjustmentt\n",
    "    g2: np.array = lmbda * theta_simple \n",
    "    \n",
    "    g: np.array = g1 + g2\n",
    "    assert g.shape == theta.shape\n",
    "    return g\n",
    "\n",
    "np.random.seed(11)\n",
    "n: int = train_x_bin_ext.shape[1]\n",
    "theta: np.array = np.random.randn(n).reshape(n, 1)\n",
    "res: np.ndarray = grad(train_x_bin_ext, train_y_bin, theta=theta, lmbda=0.2)\n",
    "assert res.shape == (785, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training algorithm will store the cost associated to every epoch into a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-16T19:12:30.150739Z",
     "start_time": "2022-03-16T19:12:30.133794Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(x_set: np.ndarray, y_set: np.ndarray, theta: np.ndarray) -> float:\n",
    "    pred: np.array = ((h(x_set, theta) >= 0.5) * 1 == y_set)\n",
    "    return 100.0 * sum(pred) / pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-16T19:12:36.022187Z",
     "start_time": "2022-03-16T19:12:33.739750Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set learning rate\n",
    "eta: float = 0.2\n",
    "\n",
    "# Set regularization coefficient\n",
    "lmbda: float = 0.5\n",
    "\n",
    "# In x, we have m instances of n features each\n",
    "# Create theta as a vector (n x 1)\n",
    "n: int = np.shape(train_x_bin_ext)[1]\n",
    "theta: np.array = np.random.randn(n).reshape(n, 1)\n",
    "\n",
    "# Do the training\n",
    "epochs: int = 40\n",
    "values: list = []\n",
    "accurracies: list = []\n",
    "for i in range(epochs):\n",
    "    theta -= eta * grad(train_x_bin_ext, train_y_bin, theta, lmbda)\n",
    "    acc = compute_accuracy(train_x_bin_ext, train_y_bin, theta)\n",
    "    values.append(cost(train_x_bin_ext, train_y_bin, theta, lmbda))\n",
    "    accurracies.append(acc)\n",
    "    \n",
    "print(\"last cost: %g\" % values[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-16T19:12:36.289224Z",
     "start_time": "2022-03-16T19:12:36.064197Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(20, 10), sharex=True)\n",
    "ax[0].plot(range(len(values)), values)\n",
    "ax[0].set_xlabel('epoch') ; ax[0].set_ylabel('cost')\n",
    "ax[0].grid()\n",
    "ax[1].plot(range(len(accurracies)), accurracies)\n",
    "ax[1].set_xlabel('epoch') ; ax[1].set_ylabel('accurracy [%]')\n",
    "ax[1].grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-16T18:33:46.408367Z",
     "start_time": "2022-03-16T18:33:46.368365Z"
    }
   },
   "outputs": [],
   "source": [
    "# we calculate the accuracy for the test set\n",
    "# this assumes we count how many predictions fit with the reality and to express this idea percentage-wise\n",
    "pred = (h(test_x_bin_ext, theta) >= 0.5) * 1 == test_y_bin\n",
    "print(\"accuracy: %2.2f%% for %d patterns\" % (accurracies[-1] , test_x_bin_ext.shape[0]))\n",
    "\n",
    "# calculating confusion matrix\n",
    "# true positive: y = 1 and pred = 1\n",
    "# true negative: y = 0 and pred = 0\n",
    "# false positive: y = 0 and pred = 1\n",
    "# false negative: y = 1 and pred = 0\n",
    "pred = (h(test_x_bin_ext, theta) >= 0.5) * 1\n",
    "tp = np.sum(np.logical_and(pred == 1, test_y_bin == 1))\n",
    "tn = np.sum(np.logical_and(pred == 0, test_y_bin == 0))\n",
    "fp = np.sum(np.logical_and(pred == 1, test_y_bin == 0))\n",
    "fn = np.sum(np.logical_and(pred == 0, test_y_bin == 1))\n",
    "\n",
    "headers: list = ['Confusion Matrix', 'pred: 0', 'pred: 1', 'pred: all'] \n",
    "table: list = [\n",
    "    ['actual: 0', tn, fp, tn + fp],\n",
    "    ['actual: 1', fn, tp, fn + tp],\n",
    "    ['actual: all', tn + fn, fp + tp, tn + fn + fp + tp]]\n",
    "tab.table(table, headers, width=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-16T19:12:58.833824Z",
     "start_time": "2022-03-16T19:12:58.453113Z"
    }
   },
   "outputs": [],
   "source": [
    "# classes\n",
    "k: int = 10\n",
    "\n",
    "# We add the ones column to features\n",
    "train_x_all_ext = add_ones_column(normalize(train_x))\n",
    "test_x_all_ext = add_ones_column(normalize(test_x))\n",
    "\n",
    "assert train_x_all_ext.shape == (60000, 785)\n",
    "assert test_x_all_ext.shape == (10000, 785)\n",
    "assert np.all(train_x_all_ext[:, 0] == 1)\n",
    "assert np.all(test_x_all_ext[:, 0] == 1)\n",
    "assert np.all(train_x_all_ext <= 1)\n",
    "assert np.all(test_x_all_ext <= 1)\n",
    "assert np.all(train_x_all_ext >= 0)\n",
    "assert np.all(test_x_all_ext >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(val: int, classes: int) -> np.ndarray:\n",
    "    \"\"\"Achieves 'one-hot encoding', the conversion of an integer at a binary array, \n",
    "    that has 1 only on the position specified by the 'val' value\n",
    "    Args:\n",
    "        val: the class that needs to be encoded, an integer between {0, 1, ... K-1}\n",
    "        classes: the number of K classes\n",
    "    Returns:\n",
    "        an array of zeroes of length K, where only on the position 'val' we have a value of 1\"\"\"\n",
    "    assert 0 <= val < classes\n",
    "    result = np.zeros(classes)\n",
    "    result[val] = 1\n",
    "    result = result.reshape(1, classes)\n",
    "    assert result.shape == (1, classes)\n",
    "    return result\n",
    "\n",
    "assert np.all(one_hot(7, k) == np.array([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]))\n",
    "assert np.all(one_hot(3, k) == np.array([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]))\n",
    "\n",
    "train_y_all = np.concatenate([one_hot(int(i), k) for i in train_y])\n",
    "test_y_all = np.concatenate([one_hot(int(i), k) for i in test_y])\n",
    "assert train_y_all.shape == (60000, 10)\n",
    "assert test_y_all.shape == (10000, 10)\n",
    "assert np.all((train_y_all != 0) == (train_y_all == 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The product $X \\theta$ among the matrix X of dimension (m, n) and $\\theta$ of dimension (n, k) will have the dimension (m, k):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prod(x: np.ndarray, theta: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Product between X of shape (m x n) and theta of shape (n x k)\n",
    "    Args:\n",
    "        x: the features, of dimension m x n\n",
    "        theta: parameters, of dimension n x k\n",
    "    Returns:\n",
    "        their product of dimension m x k\"\"\"\n",
    "    return np.dot(x, theta)\n",
    "\n",
    "m, n = train_x_all_ext.shape\n",
    "np.random.seed(11)\n",
    "theta = np.random.randn(n, k)\n",
    "assert prod(train_x_all_ext, theta).shape == (m, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $softmax()$ will have the same dimensions (m, k) and needs to give the sum 1 on every column. Its calculation can be written in a more compact way as:\n",
    "\n",
    "$$softmax(X, \\theta) = \\frac{e^{X \\theta}}{e^{X \\theta} \\cdot \\mathbb{1}_k}$$\n",
    "\n",
    "The denominator term, $e^{X \\theta} \\cdot \\mathbb{1}_k$, is not a matrix anymore, but a vector of dimension (m, 1) (essentially the computation of the sum on every lane is done). For the accomplishment of the division we do the broadcast operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x: np.ndarray, theta: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\"\n",
    "    The calculation of the softmax function\n",
    "    Args:\n",
    "        x: features, of dimension m x n\n",
    "        theta: parameters, of dimension n x k\n",
    "    Returns:\n",
    "        their product, of dimension m x k\"\"\"\n",
    "    assert x.shape[1] == theta.shape[0]\n",
    "    a: float = np.exp(prod(x, theta))\n",
    "    ones: np.array = np.ones((k, 1))\n",
    "    return a / prod(a, ones)\n",
    "\n",
    "m, n = train_x_all_ext.shape\n",
    "np.random.seed(11)\n",
    "theta: np.ndarray = np.random.randn(n, k)\n",
    "smax: np.ndarray = softmax(train_x_all_ext, theta)\n",
    "assert smax.shape == (m, k)\n",
    "assert np.all((smax.sum(axis=1) - 1) < 1e-12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function that includes the adjustment can be written in a more compact way as (note that the index $i$ starts at 1):\n",
    "\n",
    "$$J(\\theta, \\lambda) = -\\frac{1}{m} \\mathbb{1}_m^T \\left\\{ Y \\odot \\log[softmax(X \\theta)] \\mathbb{1}_k \\right\\} + \\frac{\\lambda}{2} \\sum_{i=1}^{n-1}\\sum_{j=0}^{k-1} \\theta_{i,j}^2 = -\\frac{1}{m} \\mathbb{1}_m^T \\left\\{ Y \\odot \\log[softmax(X \\theta)] \\mathbb{1}_k \\right\\} + \\frac{\\lambda}{2} \\|\\Theta[1:, :]\\|_F^2$$\n",
    "\n",
    "where forr a matrix $A$ of type $m\\times n$, $\\| A \\|_F$ is the Forbenius norm: $\\| A \\|_F = \\sqrt{\\sum\\limits_{i=0}^{m-1}\\sum\\limits_{j=0}^{n-1} |a_{ij}|^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sigma_sum(start: int, end: int, expression: any) -> any:\n",
    "#     return sum(expression(i) for i in range(start, end))\n",
    "\n",
    "def cost(x: np.ndarray, y:np.ndarray, theta: np.ndarray, lmbda: float) -> float:\n",
    "    \"\"\"The cost includes the adjustment\n",
    "    Args:\n",
    "        x: the features of dimension m x n\n",
    "        y: the classes, of dimension m x k\n",
    "        theta: parameters, of dimension n x k\n",
    "        lmbda: the parameter of adjustment, scalar\n",
    "    Returns:\n",
    "        the cost as scalar\"\"\"\n",
    "    # not working!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    # m=x.shape[0]\n",
    "    # a = (-1/m)*np.ones((m,1)).T @ (y * np.log(softmax(x, theta)) @ np.ones((k, 1)))\n",
    "    # b = (lmbda/(2*m))*np.sum(np.square(theta[1:]))\n",
    "    # # b = (lmbda/(2*m))*np.ones((n,1)).T @ np.ones((n,1))\n",
    "    # print(a + b) \n",
    "    # return a + b\n",
    "    return -np.sum(y * np.log(softmax(x, theta))) / x.shape[0] + lmbda * np.sum(theta ** 2) / (2 * x.shape[0])\n",
    "\n",
    "m, n = train_x_all_ext.shape\n",
    "np.random.seed(11)\n",
    "theta = np.random.randn(n, k)\n",
    "assert (cost(train_x_all_ext, train_y_all, theta=theta, lmbda=0.2) - 804.384938) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient is calculated as:\n",
    "\n",
    "$$\\nabla_{\\theta}J = - \\frac{1}{m} X^T \\left[Y - softmax(X \\theta) \\right] + \\lambda \\left[ \\mathbb{0}_n, \\theta_{1 \\dots k-1} \\right]$$\n",
    "\n",
    "where the matrix $\\left[ \\mathbb{0}_n, \\theta_{1 \\dots k-1} \\right]$ still has the dimension (n, k), just like $\\theta$, but the first line is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deltas(x: np.ndarray, y: np.ndarray, theta: np.ndarray, lmbda: float) -> np.ndarray:\n",
    "    \"\"\"Calculates the gradient\n",
    "    Args:\n",
    "        x: the features, of dimension m x n\n",
    "        y: the classes, of dimension m x k\n",
    "        theta: the parameters, of dimension n x k\n",
    "        lmbda: the parameter of adjustment, scalar\n",
    "    Returns:\n",
    "        the matrix of gradients, of theta's dimension (n x k)\"\"\"\n",
    "    \n",
    "    a = (-1/m) * (x.T @ (y - softmax(x, theta)))\n",
    "    theta_copy = theta.copy()\n",
    "    theta_copy[0] = 0\n",
    "    b = lmbda * theta_copy\n",
    "    return a + b\n",
    "\n",
    "m, n = train_x_all_ext.shape\n",
    "np.random.seed(11)\n",
    "theta = np.random.randn(n, k)\n",
    "grad = deltas(train_x_all_ext, train_y_all, theta=theta, lmbda=0.2)\n",
    "assert grad.shape == (n, k)\n",
    "print(grad.sum()+6.0286086)\n",
    "assert (grad.sum() + 6.0286086) < 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def calculate_accurracy(set_x, set_y, theta):\n",
    "    a = softmax(set_x, theta)\n",
    "    b = np.argmax(a, axis=1)\n",
    "    c = np.argmax(set_y, axis=1)\n",
    "    return 100.0 * accuracy_score(b, c)     # same output with or without 100.0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of classes\n",
    "k = 10\n",
    "\n",
    "lmbda, alpha = 0.05, 0.65\n",
    "m, n = train_x_all_ext.shape\n",
    "np.random.seed(11)\n",
    "theta = np.random.randn(n, k)\n",
    "\n",
    "epochs: int = 300\n",
    "values: list = []\n",
    "accurracies: list = []\n",
    "for i in range(epochs):\n",
    "    theta -= alpha * deltas(train_x_all_ext, train_y_all, theta, lmbda)\n",
    "    if (i % 10 == 0):\n",
    "        values.append(cost(train_x_all_ext, train_y_all, theta, lmbda))\n",
    "        accurracies.append(calculate_accurracy(test_x_all_ext, test_y_all, theta))\n",
    "        print(\"epoch: \", i, \"cost: \", values[-1])\n",
    "        lmbda *= 0.9\n",
    "    \n",
    "print(\"last costs: %g\" % values[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(20, 10), sharex=True)\n",
    "ax[0].plot([x * 10 for x in range(len(values))], values, 'o-')\n",
    "ax[0].set_xlabel('epoch') ; ax[0].set_ylabel('cost')\n",
    "ax[0].grid()\n",
    "ax[1].plot([x * 10 for x in range(len(accurracies))], accurracies, 'o-')\n",
    "ax[1].set_xlabel('epoch') ; ax[1].set_ylabel('accurracy [%]')\n",
    "ax[1].grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we calculate the accuracy of the test set\n",
    "# this assumes we count how many predictions fit with the reality and to express this idea in a procentually-wise way\n",
    "pred = softmax(test_x_all_ext, theta)\n",
    "actual = np.argmax(pred, axis=1)\n",
    "equalities = np.sum(pred == actual)\n",
    "print(\"Test accuracy: %2.2f%%\" % (100.0 * accuracy_score(actual, np.argmax(test_y_all, axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we calculate the vectors of the predictions as well as the vector of the true values\n",
    "pred: np.ndarray = np.argmax(softmax(test_x_all_ext, theta), axis=1)\n",
    "actual: np.ndarray = np.argmax(test_y_all, axis=1)\n",
    "\n",
    "# confusion matrix will have at the intersection line/column how many samples from the given class by the number\n",
    "# of the line have been predicted as being part of the class given by the column number\n",
    "conf_matrix: np.ndarray = np.zeros((k, k), dtype=np.int32)\n",
    "for i in range(len(pred)):\n",
    "    conf_matrix[actual[i], pred[i]] += 1\n",
    "\n",
    "assert len(conf_matrix) == k\n",
    "assert (sum(len(row) for row in conf_matrix)) == k ** 2\n",
    "\n",
    "    \n",
    "headers = ['CnfMat'] + [f'pr: {x}' for x in range(k)] + ['all a'] \n",
    "table = []\n",
    "for i in range(k):\n",
    "    table.append([f'act: {i}'] + list(conf_matrix[i]) + [sum(conf_matrix[i])])\n",
    "table.append(['all p'] + [sum(conf_matrix[:, i])\n",
    "             for i in range(k)] + [sum(sum(conf_matrix))])\n",
    "\n",
    "tab.table(table, headers, width=6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "58f10e1b20caf39f3bae4facf866a1e85e14779671631b268474442cf9b2784d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
